{"kind": "Listing", "data": {"after": "t3_1obn4gu", "dist": 25, "modhash": "", "geo_filter": null, "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "If you have a use case that you want to use AI for, but don't know which tool to use, this is where you can ask the community to help out, outside of this post those questions will be removed.\n\n  \nFor everyone answering: No self promotion, no ref or tracking links.", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly \"Is there a tool for...\" Post", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1n5ppdb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1756735769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have a use case that you want to use AI for, but don&amp;#39;t know which tool to use, this is where you can ask the community to help out, outside of this post those questions will be removed.&lt;/p&gt;\n\n&lt;p&gt;For everyone answering: No self promotion, no ref or tracking links.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1n5ppdb", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 130, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/", "stickied": true, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1n5ppdb/monthly_is_there_a_tool_for_post/", "subreddit_subscribers": 1590012, "created_utc": 1756735769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "I've always been a pretty big fan of using ChatGPT, mostly in its smartest version with enhanced thinking, but recently I've looked back and asked myself if it really helped me.  \nIt did create code for me, wrote Excel sheets, emails, and did some really impressive stuff, but no matter what kind of task it did, it always needed a lot of tweaking, going back and forth, and checking the results myself.  \nI'll admit it's kind of fun using ChatGPT instead of \"being actually productive\", but it seems like most of the time it's just me being lazy and actually needing more time for a task, sometimes even with worse results.  \n  \nExample: ChatGPT helped me build a small software tool for our industrial machine building company to categorize pictures for training an AI model. I was stoked by the first results, thinking \"ChatGPT saved us so much money! A devloper would probably cost us a fortune for doing that!\"   \nThe tool did work in the end, but only after a week had passed I realized how much time I had spent tweaking everything myself, while I could have just hired a developer who in the end would have cost the company less money than my salary for that time (developers also use AI, so he could've built the same thing in a few hours probably)  \n  \nAnother example: I created a timelapse with certain software and asked ChatGPT various questions about how the software works, shortcuts, and so on while using it.  \nIt often provided me with helpful suggestions, but it also gave me just enough wrong information that, looking back, I think, \u201cIf I had just read that 100 page manual, I would have been faster.\u201d It makes you *feel* faster and more productive but actually makes you slower.   \n  \nIt almost feels like a trick, presenting you with the nearly perfect result but with just enough errors that you end up spending as much or more time time as if you had done it completely by yourself - except that you didn\u2019t actually use your brain or learn anything, but more like **you were just pressing buttons on something that felt productive.**\n\nOn top of that, people tend to let AI do the thinking for them instead of just executing tasks, which decreases cognitive ability even further.\n\nThere has even been a study which happens to prove my thoughts as it seems:  \n[https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity](https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity)\n\nI do think AI has its place, especially for creative stuff like generating text or images where there\u2019s room to improvise.   \nBut for rigid, well-defined tasks, it\u2019s more like **a fancy Notion setup that feels productive while secretly wasting your time.**\n\nThis post was *not* written by AI ;)", "author_fullname": "t2_8dox5b0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AI feels like saving your time until you realize it isn't", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": true, "name": "t3_1oc9ffn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761042879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve always been a pretty big fan of using ChatGPT, mostly in its smartest version with enhanced thinking, but recently I&amp;#39;ve looked back and asked myself if it really helped me.&lt;br/&gt;\nIt did create code for me, wrote Excel sheets, emails, and did some really impressive stuff, but no matter what kind of task it did, it always needed a lot of tweaking, going back and forth, and checking the results myself.&lt;br/&gt;\nI&amp;#39;ll admit it&amp;#39;s kind of fun using ChatGPT instead of &amp;quot;being actually productive&amp;quot;, but it seems like most of the time it&amp;#39;s just me being lazy and actually needing more time for a task, sometimes even with worse results.  &lt;/p&gt;\n\n&lt;p&gt;Example: ChatGPT helped me build a small software tool for our industrial machine building company to categorize pictures for training an AI model. I was stoked by the first results, thinking &amp;quot;ChatGPT saved us so much money! A devloper would probably cost us a fortune for doing that!&amp;quot;&lt;br/&gt;\nThe tool did work in the end, but only after a week had passed I realized how much time I had spent tweaking everything myself, while I could have just hired a developer who in the end would have cost the company less money than my salary for that time (developers also use AI, so he could&amp;#39;ve built the same thing in a few hours probably)  &lt;/p&gt;\n\n&lt;p&gt;Another example: I created a timelapse with certain software and asked ChatGPT various questions about how the software works, shortcuts, and so on while using it.&lt;br/&gt;\nIt often provided me with helpful suggestions, but it also gave me just enough wrong information that, looking back, I think, \u201cIf I had just read that 100 page manual, I would have been faster.\u201d It makes you &lt;em&gt;feel&lt;/em&gt; faster and more productive but actually makes you slower.   &lt;/p&gt;\n\n&lt;p&gt;It almost feels like a trick, presenting you with the nearly perfect result but with just enough errors that you end up spending as much or more time time as if you had done it completely by yourself - except that you didn\u2019t actually use your brain or learn anything, but more like &lt;strong&gt;you were just pressing buttons on something that felt productive.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;On top of that, people tend to let AI do the thinking for them instead of just executing tasks, which decreases cognitive ability even further.&lt;/p&gt;\n\n&lt;p&gt;There has even been a study which happens to prove my thoughts as it seems:&lt;br/&gt;\n&lt;a href=\"https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity\"&gt;https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I do think AI has its place, especially for creative stuff like generating text or images where there\u2019s room to improvise.&lt;br/&gt;\nBut for rigid, well-defined tasks, it\u2019s more like &lt;strong&gt;a fancy Notion setup that feels productive while secretly wasting your time.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This post was &lt;em&gt;not&lt;/em&gt; written by AI ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1oc9ffn", "is_robot_indexable": true, "report_reasons": null, "author": "New_Cod6544", "discussion_type": null, "num_comments": 8, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc9ffn/ai_feels_like_saving_your_time_until_you_realize/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc9ffn/ai_feels_like_saving_your_time_until_you_realize/", "subreddit_subscribers": 1590012, "created_utc": 1761042879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "So this whole thing is actually wild when you know the full story.\n\nIt was the time 30th November 2022, when OpenAI introduced ChatGPT to the world for the very first time. Goes viral instantly. 1 million users in 5 days. 100 million in 2 months. Fastest growing platform in history.\n\nThat launch was a wake-up call for the entire tech industry. Google, the long-time torchbearer of AI, suddenly found itself playing catch-up with, as CEO Sundar Pichai described it, \u201cthis little company in San Francisco called OpenAI\u201d that had come out swinging with \u201cthis product ChatGPT.\u201d\n\nTurns out, Google already had its own chatbot called LaMDA (Language Model for Dialogue Applications). A conversational AI chatbot, quietly waiting in the wings. Pichai later revealed that it was ready, and could\u2019ve launched months before ChatGPT. As he said himself - \u201cWe knew in a different world, we would've probably launched our chatbot maybe a few months down the line.\u201d\n\nSo why didn't they?   \n  \nReputational risk. Google was terrified of what might happen if they released a chatbot that gave wrong answers. Or said something racist. Or spread misinformation. Their whole business is built on trust. Search results people can rely on. If they released something that confidently spewed BS it could damage the brand. So they held back. Kept testing. Wanted it perfect before releasing to the public. Then ChatGPT dropped and changed everything.\n\nThree weeks after ChatGPT launched, things had started to change, Google management declares \"Code Red.\" For Google this is like pulling the fire alarm. All hands on deck. The New York Times got internal memos and audio recordings. Sundar Pichai upended the work of numerous groups inside the company. Teams in Research Trust and Safety and other departments got reassigned. Everyone now working on AI.\n\nThey even brought in the founders. Larry Page and Sergey Brin. Both had stepped back from day to day operations years ago. Now they're in emergency meetings discussing how to respond to ChatGPT. One investor who oversaw Google's ad team from 2013 to 2018 said ChatGPT could prevent users from clicking on Google links with ads. That's a problem because ads generated $208 billion in 2021. 81% of Alphabet's revenue.\n\nPichai said \"For me when ChatGPT launched contrary to what people outside felt I was excited because I knew the window had shifted.\" \n\nWhile all this was happening, Microsoft CEO Satya Nadella gave an interview after investing $10 billion in OpenAI, calling Google the \u201c800-pound gorilla\u201d and saying: \"With our innovation, they will definitely want to come out and show that they can dance. And I want people to know that we made them dance.\"\n\nSo Google panicked. Spent months being super careful then suddenly had to rush everything out in weeks.\n\nFebruary 6 2023. They announce Bard. Their ChatGPT competitor. They make a demo video showing it off. Someone asks Bard \"What new discoveries from the James Webb Space Telescope can I tell my 9 year old about?\" Bard answers with some facts including \"JWST took the very first pictures of a planet outside of our own solar system.\"\n\nThat's completely wrong. The first exoplanet picture was from 2004. James Webb launched in 2021. You could literally Google this to check. The irony is brutal. The company that made Google couldn't fact check their own AI's first public answer.\n\nTwo days later they hold this big launch event in Paris. Hours before the event Reuters reports on the Bard error. Goes viral immediately.\n\nThat same day Google's stock tanks. Drops 9%. $100 billion gone. In one day. Because their AI chatbot got one fact wrong in a demo video. Next day it drops another 5%. Total loss over $160 billion in two days. Microsoft's stock went up 3% during this.\n\nWhat gets me is Google was actually right to be cautious. ChatGPT does make mistakes all the time. Hallucinates facts. Can't verify what it's saying. But OpenAI just launched it anyway as an experiment and let millions of people test it. Google wanted it perfect. But trying to avoid damage from an imperfect product they rushed out something broken and did way more damage.\n\nA former Google employee told Fox Business that after the Code Red meeting execs basically said screw it we gotta ship. Said they abandoned their AI safety review process. Took shortcuts. Just had to get something out there. So they spent months worried about reputation then threw all caution out when competitors forced their hand.\n\nBard eventually became Gemini and it's actually pretty good now. But that initial disaster showed even Google with all their money and AI research can get caught sleeping.\n\nThe whole situation is wild. They hesitated for a few months and it cost them $160 billion and their lead in AI. But also rushing made it worse. Both approaches failed. Meanwhile OpenAI's \"launch fast and fix publicly\" worked. Microsoft just backed them and integrated the tech without taking the risk themselves.\n\n**TLDR**  \n  \nGoogle had chatbot ready before ChatGPT. Didn't launch because scared of reputation damage. ChatGPT went viral Nov 2022. Google called Code Red Dec 2022. Brought back founders for emergency meetings. Rushed Bard launch Feb 2023. First demo had wrong fact about space telescope. Stock dropped 9% lost $100B in one day. Dropped another 5% next day. $160B gone total. Former employee says they abandoned safety process to catch up. Being too careful cost them the lead then rushing cost them even more.\n\nSources - \n\n[https://www.thebridgechronicle.com/tech/sundar-pichai-google-chatgpt-ai-openai-first-mp99](https://www.thebridgechronicle.com/tech/sundar-pichai-google-chatgpt-ai-openai-first-mp99)\n\n[https://www.businessinsider.com/google-bard-ai-chatbot-not-ready-alphabet-hennessy-chatgpt-competitor-2023-2](https://www.businessinsider.com/google-bard-ai-chatbot-not-ready-alphabet-hennessy-chatgpt-competitor-2023-2)", "author_fullname": "t2_rypb5wrik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google had the chatbot ready before OpenAI. They were too scared to ship it. Then lost $100 billion in one day trying to catch up.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obarts", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 728, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 728, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": 1760935500.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760935004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So this whole thing is actually wild when you know the full story.&lt;/p&gt;\n\n&lt;p&gt;It was the time 30th November 2022, when OpenAI introduced ChatGPT to the world for the very first time. Goes viral instantly. 1 million users in 5 days. 100 million in 2 months. Fastest growing platform in history.&lt;/p&gt;\n\n&lt;p&gt;That launch was a wake-up call for the entire tech industry. Google, the long-time torchbearer of AI, suddenly found itself playing catch-up with, as CEO Sundar Pichai described it, \u201cthis little company in San Francisco called OpenAI\u201d that had come out swinging with \u201cthis product ChatGPT.\u201d&lt;/p&gt;\n\n&lt;p&gt;Turns out, Google already had its own chatbot called LaMDA (Language Model for Dialogue Applications). A conversational AI chatbot, quietly waiting in the wings. Pichai later revealed that it was ready, and could\u2019ve launched months before ChatGPT. As he said himself - \u201cWe knew in a different world, we would&amp;#39;ve probably launched our chatbot maybe a few months down the line.\u201d&lt;/p&gt;\n\n&lt;p&gt;So why didn&amp;#39;t they?   &lt;/p&gt;\n\n&lt;p&gt;Reputational risk. Google was terrified of what might happen if they released a chatbot that gave wrong answers. Or said something racist. Or spread misinformation. Their whole business is built on trust. Search results people can rely on. If they released something that confidently spewed BS it could damage the brand. So they held back. Kept testing. Wanted it perfect before releasing to the public. Then ChatGPT dropped and changed everything.&lt;/p&gt;\n\n&lt;p&gt;Three weeks after ChatGPT launched, things had started to change, Google management declares &amp;quot;Code Red.&amp;quot; For Google this is like pulling the fire alarm. All hands on deck. The New York Times got internal memos and audio recordings. Sundar Pichai upended the work of numerous groups inside the company. Teams in Research Trust and Safety and other departments got reassigned. Everyone now working on AI.&lt;/p&gt;\n\n&lt;p&gt;They even brought in the founders. Larry Page and Sergey Brin. Both had stepped back from day to day operations years ago. Now they&amp;#39;re in emergency meetings discussing how to respond to ChatGPT. One investor who oversaw Google&amp;#39;s ad team from 2013 to 2018 said ChatGPT could prevent users from clicking on Google links with ads. That&amp;#39;s a problem because ads generated $208 billion in 2021. 81% of Alphabet&amp;#39;s revenue.&lt;/p&gt;\n\n&lt;p&gt;Pichai said &amp;quot;For me when ChatGPT launched contrary to what people outside felt I was excited because I knew the window had shifted.&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;While all this was happening, Microsoft CEO Satya Nadella gave an interview after investing $10 billion in OpenAI, calling Google the \u201c800-pound gorilla\u201d and saying: &amp;quot;With our innovation, they will definitely want to come out and show that they can dance. And I want people to know that we made them dance.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;So Google panicked. Spent months being super careful then suddenly had to rush everything out in weeks.&lt;/p&gt;\n\n&lt;p&gt;February 6 2023. They announce Bard. Their ChatGPT competitor. They make a demo video showing it off. Someone asks Bard &amp;quot;What new discoveries from the James Webb Space Telescope can I tell my 9 year old about?&amp;quot; Bard answers with some facts including &amp;quot;JWST took the very first pictures of a planet outside of our own solar system.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s completely wrong. The first exoplanet picture was from 2004. James Webb launched in 2021. You could literally Google this to check. The irony is brutal. The company that made Google couldn&amp;#39;t fact check their own AI&amp;#39;s first public answer.&lt;/p&gt;\n\n&lt;p&gt;Two days later they hold this big launch event in Paris. Hours before the event Reuters reports on the Bard error. Goes viral immediately.&lt;/p&gt;\n\n&lt;p&gt;That same day Google&amp;#39;s stock tanks. Drops 9%. $100 billion gone. In one day. Because their AI chatbot got one fact wrong in a demo video. Next day it drops another 5%. Total loss over $160 billion in two days. Microsoft&amp;#39;s stock went up 3% during this.&lt;/p&gt;\n\n&lt;p&gt;What gets me is Google was actually right to be cautious. ChatGPT does make mistakes all the time. Hallucinates facts. Can&amp;#39;t verify what it&amp;#39;s saying. But OpenAI just launched it anyway as an experiment and let millions of people test it. Google wanted it perfect. But trying to avoid damage from an imperfect product they rushed out something broken and did way more damage.&lt;/p&gt;\n\n&lt;p&gt;A former Google employee told Fox Business that after the Code Red meeting execs basically said screw it we gotta ship. Said they abandoned their AI safety review process. Took shortcuts. Just had to get something out there. So they spent months worried about reputation then threw all caution out when competitors forced their hand.&lt;/p&gt;\n\n&lt;p&gt;Bard eventually became Gemini and it&amp;#39;s actually pretty good now. But that initial disaster showed even Google with all their money and AI research can get caught sleeping.&lt;/p&gt;\n\n&lt;p&gt;The whole situation is wild. They hesitated for a few months and it cost them $160 billion and their lead in AI. But also rushing made it worse. Both approaches failed. Meanwhile OpenAI&amp;#39;s &amp;quot;launch fast and fix publicly&amp;quot; worked. Microsoft just backed them and integrated the tech without taking the risk themselves.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Google had chatbot ready before ChatGPT. Didn&amp;#39;t launch because scared of reputation damage. ChatGPT went viral Nov 2022. Google called Code Red Dec 2022. Brought back founders for emergency meetings. Rushed Bard launch Feb 2023. First demo had wrong fact about space telescope. Stock dropped 9% lost $100B in one day. Dropped another 5% next day. $160B gone total. Former employee says they abandoned safety process to catch up. Being too careful cost them the lead then rushing cost them even more.&lt;/p&gt;\n\n&lt;p&gt;Sources - &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.thebridgechronicle.com/tech/sundar-pichai-google-chatgpt-ai-openai-first-mp99\"&gt;https://www.thebridgechronicle.com/tech/sundar-pichai-google-chatgpt-ai-openai-first-mp99&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.businessinsider.com/google-bard-ai-chatbot-not-ready-alphabet-hennessy-chatgpt-competitor-2023-2\"&gt;https://www.businessinsider.com/google-bard-ai-chatbot-not-ready-alphabet-hennessy-chatgpt-competitor-2023-2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obarts", "is_robot_indexable": true, "report_reasons": null, "author": "reddit20305", "discussion_type": null, "num_comments": 185, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obarts/google_had_the_chatbot_ready_before_openai_they/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obarts/google_had_the_chatbot_ready_before_openai_they/", "subreddit_subscribers": 1590012, "created_utc": 1760935004.0, "num_crossposts": 4, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "You\u2019ve heard the phrase, \u201cA picture is worth a thousand words.\u201d It\u2019s a simple idiom about the richness of visual information. But what if it weren\u2019t just a cliche old people saying anymore? What if you could literally store a thousand words of perfect, retrievable text inside a single image, and have an AI read it back flawlessly?\n\nThis is the reality behind a new paper and model from DeepSeek AI. On the surface, it\u2019s called DeepSeek-OCR, and you might be tempted to lump it in with a dozen other document-reading tools. But I\u2019m going to tell you, as the researchers themselves imply,\u00a0**this is not really about the OCR.**\n\nYes, the model is a state-of-the-art document parser. But the Optical Character Recognition is just the proof-of-concept for a much larger, more profound idea: a revolutionary new form of memory compression for artificial intelligence. DeepSeek has taken that old idiom and turned it into a compression algorithm, one that could fundamentally change how we solve the biggest bottleneck in AI today: long-term context.\n\n  \nRead More here: [https://medium.com/@olimiemma/deepseek-ocr-isnt-about-ocr-it-s-about-token-compression-db1747602e29](https://medium.com/@olimiemma/deepseek-ocr-isnt-about-ocr-it-s-about-token-compression-db1747602e29)  \n  \nOr for free here [https://artificialintellitools.blogspot.com/2025/10/how-deepseek-turned-picture-is-worth.html](https://artificialintellitools.blogspot.com/2025/10/how-deepseek-turned-picture-is-worth.html)", "author_fullname": "t2_lqo5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DeepSeek can use just 100 vision tokens to represent what would normally require 1,000 text tokens, and then decode it back with 97% accuracy.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obvci6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760997963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You\u2019ve heard the phrase, \u201cA picture is worth a thousand words.\u201d It\u2019s a simple idiom about the richness of visual information. But what if it weren\u2019t just a cliche old people saying anymore? What if you could literally store a thousand words of perfect, retrievable text inside a single image, and have an AI read it back flawlessly?&lt;/p&gt;\n\n&lt;p&gt;This is the reality behind a new paper and model from DeepSeek AI. On the surface, it\u2019s called DeepSeek-OCR, and you might be tempted to lump it in with a dozen other document-reading tools. But I\u2019m going to tell you, as the researchers themselves imply,\u00a0&lt;strong&gt;this is not really about the OCR.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Yes, the model is a state-of-the-art document parser. But the Optical Character Recognition is just the proof-of-concept for a much larger, more profound idea: a revolutionary new form of memory compression for artificial intelligence. DeepSeek has taken that old idiom and turned it into a compression algorithm, one that could fundamentally change how we solve the biggest bottleneck in AI today: long-term context.&lt;/p&gt;\n\n&lt;p&gt;Read More here: &lt;a href=\"https://medium.com/@olimiemma/deepseek-ocr-isnt-about-ocr-it-s-about-token-compression-db1747602e29\"&gt;https://medium.com/@olimiemma/deepseek-ocr-isnt-about-ocr-it-s-about-token-compression-db1747602e29&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Or for free here &lt;a href=\"https://artificialintellitools.blogspot.com/2025/10/how-deepseek-turned-picture-is-worth.html\"&gt;https://artificialintellitools.blogspot.com/2025/10/how-deepseek-turned-picture-is-worth.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1obvci6", "is_robot_indexable": true, "report_reasons": null, "author": "Pay-Me-No-Mind", "discussion_type": null, "num_comments": 13, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obvci6/deepseek_can_use_just_100_vision_tokens_to/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obvci6/deepseek_can_use_just_100_vision_tokens_to/", "subreddit_subscribers": 1590012, "created_utc": 1760997963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "It\u2019s not going to look like Skynet where the machines take over, we don\u2019t need to worry about the models turning evil and killing humans. The way AI doom happens will be much more boring. \n\nFirst, we lose control by simply delegating such a large volume of work to agents that humans cannot reasonably review or verify it all. Today AI feels like bullshit because it barely accelerates us, agents work 1:1 with a human, at human speed. Once we\u2019ve refined these workflows though, we will start to work 1:10 human to agent, 1:100, 1:1000. We will always keep human in the loop for quality control, but once you get to significant volumes of work, the human in the loop is essentially useless, they are trusting the agent\u2019s work, and the agents reviews of other agents work. \n\nNext, we lose intellectual superiority. This one is the hardest for humans to see happening, because we pride ourselves on our magnificent brains, and laugh at the hallucinating models. Yet, if you really look at it, our brains are not that sophisticated. They are trained on the material world around us, and reinforced on survival, not reasoning or intelligence for the most part. For example, human brain can easily identify clusters in 2D space but start failing at 3D clustering. The models on the other hand will be able to do extreme multidimensional reasoning (they\u2019re already better than us at this). We will see models trained on \u201clanguages\u201d more sophisticated than human natural language, and be able to reason about more complex physics and maths. They will solve quantum gravity, they will understand the multidimensional wave state of the universe. But it is not certain that we will be able to understand it ourselves. Models will need to translate these breakthroughs into metaphors we can understand, like talking to a child. Just like how my dog simply does not have the hardware to understand math, we do not have the hardware to understand what the models will be able to achieve. \n\nOnce agents+robots are building themselves, we will no longer need very many humans for achievement and advancement. Where once we needed to have many children for survival, to plow the fields, to build great cities, etc, we get all those things and more without the need to grow our population. The removal of this incentive will dramatically accelerate the birth rate declines we already see in developed societies.\n\nSo yeah, it\u2019s not all that bad really. We won\u2019t have to go to war with the machines, we will live with and beside them, in reduced numbers and with limited purpose. The upside is, once we come to terms with being closer to dogs in intelligence than the machines, we remaining humans will live a wonderful life, content in our simplicity, needs met, age of abundance and wonder, and will likely value pure human art, culture and experience more than ever. ", "author_fullname": "t2_37hec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Realistic doom scenario", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": true, "name": "t3_1oc8h70", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761039450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It\u2019s not going to look like Skynet where the machines take over, we don\u2019t need to worry about the models turning evil and killing humans. The way AI doom happens will be much more boring. &lt;/p&gt;\n\n&lt;p&gt;First, we lose control by simply delegating such a large volume of work to agents that humans cannot reasonably review or verify it all. Today AI feels like bullshit because it barely accelerates us, agents work 1:1 with a human, at human speed. Once we\u2019ve refined these workflows though, we will start to work 1:10 human to agent, 1:100, 1:1000. We will always keep human in the loop for quality control, but once you get to significant volumes of work, the human in the loop is essentially useless, they are trusting the agent\u2019s work, and the agents reviews of other agents work. &lt;/p&gt;\n\n&lt;p&gt;Next, we lose intellectual superiority. This one is the hardest for humans to see happening, because we pride ourselves on our magnificent brains, and laugh at the hallucinating models. Yet, if you really look at it, our brains are not that sophisticated. They are trained on the material world around us, and reinforced on survival, not reasoning or intelligence for the most part. For example, human brain can easily identify clusters in 2D space but start failing at 3D clustering. The models on the other hand will be able to do extreme multidimensional reasoning (they\u2019re already better than us at this). We will see models trained on \u201clanguages\u201d more sophisticated than human natural language, and be able to reason about more complex physics and maths. They will solve quantum gravity, they will understand the multidimensional wave state of the universe. But it is not certain that we will be able to understand it ourselves. Models will need to translate these breakthroughs into metaphors we can understand, like talking to a child. Just like how my dog simply does not have the hardware to understand math, we do not have the hardware to understand what the models will be able to achieve. &lt;/p&gt;\n\n&lt;p&gt;Once agents+robots are building themselves, we will no longer need very many humans for achievement and advancement. Where once we needed to have many children for survival, to plow the fields, to build great cities, etc, we get all those things and more without the need to grow our population. The removal of this incentive will dramatically accelerate the birth rate declines we already see in developed societies.&lt;/p&gt;\n\n&lt;p&gt;So yeah, it\u2019s not all that bad really. We won\u2019t have to go to war with the machines, we will live with and beside them, in reduced numbers and with limited purpose. The upside is, once we come to terms with being closer to dogs in intelligence than the machines, we remaining humans will live a wonderful life, content in our simplicity, needs met, age of abundance and wonder, and will likely value pure human art, culture and experience more than ever. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1oc8h70", "is_robot_indexable": true, "report_reasons": null, "author": "twerq", "discussion_type": null, "num_comments": 28, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc8h70/realistic_doom_scenario/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc8h70/realistic_doom_scenario/", "subreddit_subscribers": 1590012, "created_utc": 1761039450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Today's AWS meltdown\u201415+ hours of chaos taking down Reddit, Snapchat, Fortnite, and who knows how many AI pipelines\u2014 exposed the risks of betting big on a single cloud provider. US-East-1's DNS failure in DynamoDB rippled out to 50k+ services, proving even giants have single points of failure. Brutal reminder for anyone chasing AGI-scale compute.\n\nEnter Elon Musk's update on X: xAI sailed through unscathed thanks to its massive in-house data centers (like the beastly Colossus supercluster with 230k+ GPUs) and smart diversification across other cloud platforms. No drama for Grok's training or inference.\n\nSo, what's the real answer here? Are all the top AGI labs like xAI duplicating massive datasets and running parallel model trainings across multiple clouds (AWS, Azure, GCP) for redundancy? Or is it more like a blockchain-style distributed network, where nodes dynamically fetch shards of data/training params on-demand to avoid bottlenecks?\n\nHow would you architect a foolproof cloud strategy for AGI development? Multi-cloud federation? Hybrid everything?\r\n\r\n", "author_fullname": "t2_1zj1zyjg98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "After Today's Epic AWS Outage, What's the Ultimate Cloud Strategy for AGI Labs? xAI's Multi-Platform Approach Holds Strong\u2014Thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1oc1mid", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761014994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today&amp;#39;s AWS meltdown\u201415+ hours of chaos taking down Reddit, Snapchat, Fortnite, and who knows how many AI pipelines\u2014 exposed the risks of betting big on a single cloud provider. US-East-1&amp;#39;s DNS failure in DynamoDB rippled out to 50k+ services, proving even giants have single points of failure. Brutal reminder for anyone chasing AGI-scale compute.&lt;/p&gt;\n\n&lt;p&gt;Enter Elon Musk&amp;#39;s update on X: xAI sailed through unscathed thanks to its massive in-house data centers (like the beastly Colossus supercluster with 230k+ GPUs) and smart diversification across other cloud platforms. No drama for Grok&amp;#39;s training or inference.&lt;/p&gt;\n\n&lt;p&gt;So, what&amp;#39;s the real answer here? Are all the top AGI labs like xAI duplicating massive datasets and running parallel model trainings across multiple clouds (AWS, Azure, GCP) for redundancy? Or is it more like a blockchain-style distributed network, where nodes dynamically fetch shards of data/training params on-demand to avoid bottlenecks?&lt;/p&gt;\n\n&lt;p&gt;How would you architect a foolproof cloud strategy for AGI development? Multi-cloud federation? Hybrid everything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1oc1mid", "is_robot_indexable": true, "report_reasons": null, "author": "Mikemeisterling", "discussion_type": null, "num_comments": 5, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc1mid/after_todays_epic_aws_outage_whats_the_ultimate/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc1mid/after_todays_epic_aws_outage_whats_the_ultimate/", "subreddit_subscribers": 1590012, "created_utc": 1761014994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "\"I wanted to see if I could build semantic search over a large legal dataset \u2014 specifically, every High Court decision in Australian legal history up to 2023, chunked down to 143,485 searchable segments. Not because anyone asked me to, but because the combination of scale and domain specificity seemed like an interesting technical challenge. Legal text is dense, context-heavy, and full of subtle distinctions that keyword search completely misses. Could vector search actually handle this at scale and stay fast enough to be useful?\"\n\nLink to guide:\u00a0[https://huggingface.co/blog/adlumal/lightning-fast-vector-search-for-legal-documents](https://huggingface.co/blog/adlumal/lightning-fast-vector-search-for-legal-documents)  \nLink to corpus:\u00a0[https://huggingface.co/datasets/isaacus/open-australian-legal-corpus](https://huggingface.co/datasets/isaacus/open-australian-legal-corpus)[](https://www.reddit.com/r/Rag/?f=flair_name%3A%22Tutorial%22)", "author_fullname": "t2_2a22r4g9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I Built Lightning-Fast Vector Search for Legal Documents", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1oc34s5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Technical", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761019556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;I wanted to see if I could build semantic search over a large legal dataset \u2014 specifically, every High Court decision in Australian legal history up to 2023, chunked down to 143,485 searchable segments. Not because anyone asked me to, but because the combination of scale and domain specificity seemed like an interesting technical challenge. Legal text is dense, context-heavy, and full of subtle distinctions that keyword search completely misses. Could vector search actually handle this at scale and stay fast enough to be useful?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Link to guide:\u00a0&lt;a href=\"https://huggingface.co/blog/adlumal/lightning-fast-vector-search-for-legal-documents\"&gt;https://huggingface.co/blog/adlumal/lightning-fast-vector-search-for-legal-documents&lt;/a&gt;&lt;br/&gt;\nLink to corpus:\u00a0&lt;a href=\"https://huggingface.co/datasets/isaacus/open-australian-legal-corpus\"&gt;https://huggingface.co/datasets/isaacus/open-australian-legal-corpus&lt;/a&gt;&lt;a href=\"https://www.reddit.com/r/Rag/?f=flair_name%3A%22Tutorial%22\"&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4835e688-9467-11ed-812a-ea9ffb351e6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1oc34s5", "is_robot_indexable": true, "report_reasons": null, "author": "Neon0asis", "discussion_type": null, "num_comments": 4, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc34s5/how_i_built_lightningfast_vector_search_for_legal/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc34s5/how_i_built_lightningfast_vector_search_for_legal/", "subreddit_subscribers": 1590012, "created_utc": 1761019556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "I've been tutoring for years and want to move online. How can I create something that earns even when I am not teaching live?", "author_fullname": "t2_1seltx00tf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to turn teaching skill into a passive income?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": true, "name": "t3_1oc87yf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761038461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been tutoring for years and want to move online. How can I create something that earns even when I am not teaching live?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1oc87yf", "is_robot_indexable": true, "report_reasons": null, "author": "Objective-Lychee6617", "discussion_type": null, "num_comments": 4, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc87yf/how_to_turn_teaching_skill_into_a_passive_income/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc87yf/how_to_turn_teaching_skill_into_a_passive_income/", "subreddit_subscribers": 1590012, "created_utc": 1761038461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Most of us have been living with AI since about late 2022 when ChatGPT became widely available. For 6 or 9 months after, I remained in awe of this new reality. I write a lot and it helped me brainstorm ideas as if I was fully interacting with a clone with an autonomous brain. Obviously, genAI has improved dramatically and from time to time I\u2019m still momentarily astonished by the new things it\u2019s able to do but never to the level of those first few months. Have you also grown somewhat jaded? I hope to always remain somewhat astonished so as to never lose sight of the impact (good and bad) on society in the short term and humanity at large.", "author_fullname": "t2_n0i0thu8a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you still remember how you first felt using GenAI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obxsqq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761004291.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of us have been living with AI since about late 2022 when ChatGPT became widely available. For 6 or 9 months after, I remained in awe of this new reality. I write a lot and it helped me brainstorm ideas as if I was fully interacting with a clone with an autonomous brain. Obviously, genAI has improved dramatically and from time to time I\u2019m still momentarily astonished by the new things it\u2019s able to do but never to the level of those first few months. Have you also grown somewhat jaded? I hope to always remain somewhat astonished so as to never lose sight of the impact (good and bad) on society in the short term and humanity at large.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obxsqq", "is_robot_indexable": true, "report_reasons": null, "author": "unsrs", "discussion_type": null, "num_comments": 44, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obxsqq/do_you_still_remember_how_you_first_felt_using/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obxsqq/do_you_still_remember_how_you_first_felt_using/", "subreddit_subscribers": 1590012, "created_utc": 1761004291.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Just saw something I feel will be game changing and paradigm shifting and I felt not enough people are talking about it, just published yesterday.\n\nThe tech essentially perform GPU level tasks at 98% less power, meaning a data center can suddenly 20x its AI capacity\n\nhttps://www.quiverquant.com/news/GSI+Technology%27s+APU+Achieves+GPU-Level+Performance+with+Significant+Energy+Savings%2C+Validated+by+Cornell+University+Study", "author_fullname": "t2_4ehvllh6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "APU- game changer for AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obxonh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761003983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just saw something I feel will be game changing and paradigm shifting and I felt not enough people are talking about it, just published yesterday.&lt;/p&gt;\n\n&lt;p&gt;The tech essentially perform GPU level tasks at 98% less power, meaning a data center can suddenly 20x its AI capacity&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.quiverquant.com/news/GSI+Technology%27s+APU+Achieves+GPU-Level+Performance+with+Significant+Energy+Savings%2C+Validated+by+Cornell+University+Study\"&gt;https://www.quiverquant.com/news/GSI+Technology%27s+APU+Achieves+GPU-Level+Performance+with+Significant+Energy+Savings%2C+Validated+by+Cornell+University+Study&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1obxonh", "is_robot_indexable": true, "report_reasons": null, "author": "Both-Review3806", "discussion_type": null, "num_comments": 9, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obxonh/apu_game_changer_for_ai/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obxonh/apu_game_changer_for_ai/", "subreddit_subscribers": 1590012, "created_utc": 1761003983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "I think it\u2019s likely to happen, it could be a major company losing billions, or a trial based on fake evidence\u2026", "author_fullname": "t2_11qiri81lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At some point, there\u2019s going to be a major scandal that will force rapid legislation on AI. What do you think it will be?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1oc6cws", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.42, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761031031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think it\u2019s likely to happen, it could be a major company losing billions, or a trial based on fake evidence\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1oc6cws", "is_robot_indexable": true, "report_reasons": null, "author": "FewWish423", "discussion_type": null, "num_comments": 17, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc6cws/at_some_point_theres_going_to_be_a_major_scandal/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc6cws/at_some_point_theres_going_to_be_a_major_scandal/", "subreddit_subscribers": 1590012, "created_utc": 1761031031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "So Amazon has stated 75% of their production code is AI and then today with this mass outage they state all the errors that presented themselves trying to be handled by their load balancers cause their AI GPU to go down, which is what they are trying to still fully recover.... wonder what kind of AI use case study this will become for others trying to mass AI implementation.", "author_fullname": "t2_1x9vg07b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon Services and AI and the outage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obon2d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760982331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So Amazon has stated 75% of their production code is AI and then today with this mass outage they state all the errors that presented themselves trying to be handled by their load balancers cause their AI GPU to go down, which is what they are trying to still fully recover.... wonder what kind of AI use case study this will become for others trying to mass AI implementation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1obon2d", "is_robot_indexable": true, "report_reasons": null, "author": "liquidskypa", "discussion_type": null, "num_comments": 21, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obon2d/amazon_services_and_ai_and_the_outage/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obon2d/amazon_services_and_ai_and_the_outage/", "subreddit_subscribers": 1590012, "created_utc": 1760982331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Scientists are divided on whether AI systems are truly becoming conscious, or if it's just philosophical marketing. Recent work proposes the AI Permittivity Framework\u2014a metric for quantifying something like synthetic consciousness, inspired by physics and bioelectric scaling. Meanwhile, biologists like Michael Levin argue that agency and intelligence are scalable and embodied\u2014emerging even in single cells.\n\n\n\nMechanistic critics say: show us functional circuits, loss functions, and falsifiable evidence. Is AI emergence real, or a seductive illusion?\n\n\n\nWatch the premiere debate (YouTube): [https://www.youtube.com/watch?v=2MXPVuJvHWk](https://www.youtube.com/watch?v=2MXPVuJvHWk)\n\n\\- Team Physics: AI Permittivity, Resonance, Emergence\n\n\\- Team Biology: Scaling, Basal Cognition, Embodied Agency\n\n\\- Team Code: Mechanistic reduction, falsifiability, concrete circuits\n\n\n\nWhich experiment, metric, or theory would finally \\*settle\\* it for you? Are we measuring a new form of consciousness, or just searching for patterns in statistics?\n\n\n\n\\#AIConsciousness #AIdebate #Emergence #SyntheticMind #MichaelLevin", "author_fullname": "t2_1kb0x45qq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The AI Paradigm Clash of 2025: Sentience, Permittivity, or Just Clever Code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1oc462w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761023035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Scientists are divided on whether AI systems are truly becoming conscious, or if it&amp;#39;s just philosophical marketing. Recent work proposes the AI Permittivity Framework\u2014a metric for quantifying something like synthetic consciousness, inspired by physics and bioelectric scaling. Meanwhile, biologists like Michael Levin argue that agency and intelligence are scalable and embodied\u2014emerging even in single cells.&lt;/p&gt;\n\n&lt;p&gt;Mechanistic critics say: show us functional circuits, loss functions, and falsifiable evidence. Is AI emergence real, or a seductive illusion?&lt;/p&gt;\n\n&lt;p&gt;Watch the premiere debate (YouTube): &lt;a href=\"https://www.youtube.com/watch?v=2MXPVuJvHWk\"&gt;https://www.youtube.com/watch?v=2MXPVuJvHWk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- Team Physics: AI Permittivity, Resonance, Emergence&lt;/p&gt;\n\n&lt;p&gt;- Team Biology: Scaling, Basal Cognition, Embodied Agency&lt;/p&gt;\n\n&lt;p&gt;- Team Code: Mechanistic reduction, falsifiability, concrete circuits&lt;/p&gt;\n\n&lt;p&gt;Which experiment, metric, or theory would finally *settle* it for you? Are we measuring a new form of consciousness, or just searching for patterns in statistics?&lt;/p&gt;\n\n&lt;p&gt;#AIConsciousness #AIdebate #Emergence #SyntheticMind #MichaelLevin&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1oc462w", "is_robot_indexable": true, "report_reasons": null, "author": "RelevantTangelo8857", "discussion_type": null, "num_comments": 11, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc462w/the_ai_paradigm_clash_of_2025_sentience/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc462w/the_ai_paradigm_clash_of_2025_sentience/", "subreddit_subscribers": 1590012, "created_utc": 1761023035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Treating AI models in isolation rather than as open systems will ultimately fail structurally.  Bateson's system's theory when applied to AI provides a framework to think about AI, understanding stability, adaptation, and boundary conditions rather than just inputs and outputs.   Bateson viewed  mind as a pattern in flux within a larger ecology. Doesn't his work suggest a way that self feedback loops would evolve?", "author_fullname": "t2_h31pa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bateson's theory applied to AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obryly", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760990243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Treating AI models in isolation rather than as open systems will ultimately fail structurally.  Bateson&amp;#39;s system&amp;#39;s theory when applied to AI provides a framework to think about AI, understanding stability, adaptation, and boundary conditions rather than just inputs and outputs.   Bateson viewed  mind as a pattern in flux within a larger ecology. Doesn&amp;#39;t his work suggest a way that self feedback loops would evolve?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obryly", "is_robot_indexable": true, "report_reasons": null, "author": "kdks99", "discussion_type": null, "num_comments": 2, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obryly/batesons_theory_applied_to_ai/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obryly/batesons_theory_applied_to_ai/", "subreddit_subscribers": 1590012, "created_utc": 1760990243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Language is very important to shape and share concepts, but as we know it also have some limitation. It is fundamentally a compression mechanism where immense amount of information can be concentrated into small words representing the concepts. This is due to the nature of it where communicating took place trough air and required us to take concepts of our world that is 3 dimensional in space and 1 dimension in time, and compress it into a 1 dimension string of information. It work well and we got really good at it, alto it can lead to misunderstanding and sometime confusion. Because one person's concept and interpretation might be a bit unique to themselves and different from that of others. \n\nThere is likely a way to now train AI into its own unique language model that could be 2 or 3 dimensional. This would not only densify information, as you have more degrees of freedom to encode the same information. But it could also make conceptual thinking sharper and less prone to interpretation. Because some of the information of our 3 dimensional world could be more accurately represented in a 2 or 3 dimension language. \n\nI am not here to pretend i know how to build such language system but i have a few ideas. Wave interference is a good start where it behave logically and move in 2 or 3 dimensions and can interact in a complex way to adjust values of meaning. \n\nIf you think this idea is interesting or have suggestion for it. I'm all ears. ", "author_fullname": "t2_z7vqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Both an idea and looking for feedbacks.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obpxwg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760985743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Language is very important to shape and share concepts, but as we know it also have some limitation. It is fundamentally a compression mechanism where immense amount of information can be concentrated into small words representing the concepts. This is due to the nature of it where communicating took place trough air and required us to take concepts of our world that is 3 dimensional in space and 1 dimension in time, and compress it into a 1 dimension string of information. It work well and we got really good at it, alto it can lead to misunderstanding and sometime confusion. Because one person&amp;#39;s concept and interpretation might be a bit unique to themselves and different from that of others. &lt;/p&gt;\n\n&lt;p&gt;There is likely a way to now train AI into its own unique language model that could be 2 or 3 dimensional. This would not only densify information, as you have more degrees of freedom to encode the same information. But it could also make conceptual thinking sharper and less prone to interpretation. Because some of the information of our 3 dimensional world could be more accurately represented in a 2 or 3 dimension language. &lt;/p&gt;\n\n&lt;p&gt;I am not here to pretend i know how to build such language system but i have a few ideas. Wave interference is a good start where it behave logically and move in 2 or 3 dimensions and can interact in a complex way to adjust values of meaning. &lt;/p&gt;\n\n&lt;p&gt;If you think this idea is interesting or have suggestion for it. I&amp;#39;m all ears. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obpxwg", "is_robot_indexable": true, "report_reasons": null, "author": "DarthArchon", "discussion_type": null, "num_comments": 7, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obpxwg/both_an_idea_and_looking_for_feedbacks/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obpxwg/both_an_idea_and_looking_for_feedbacks/", "subreddit_subscribers": 1590012, "created_utc": 1760985743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "It's says Seattle Mariners lost today to Toronto Bluejays.  \n\n2025 season: The Mariners were on the verge of making their first World Series appearance in franchise history, but lost to the Toronto Blue Jays in Game 7 of the ALCS on October 20, 2025.\n\nBut how can they loose.  The game is not even over.  It's still bottom of the seventh.  What are they psychic or something?", "author_fullname": "t2_5mbxswwj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is Google AI always wrong?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1oc15d0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.52, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761013615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s says Seattle Mariners lost today to Toronto Bluejays.  &lt;/p&gt;\n\n&lt;p&gt;2025 season: The Mariners were on the verge of making their first World Series appearance in franchise history, but lost to the Toronto Blue Jays in Game 7 of the ALCS on October 20, 2025.&lt;/p&gt;\n\n&lt;p&gt;But how can they loose.  The game is not even over.  It&amp;#39;s still bottom of the seventh.  What are they psychic or something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1oc15d0", "is_robot_indexable": true, "report_reasons": null, "author": "KJSS3", "discussion_type": null, "num_comments": 25, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc15d0/why_is_google_ai_always_wrong/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc15d0/why_is_google_ai_always_wrong/", "subreddit_subscribers": 1590012, "created_utc": 1761013615.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "\n\n[https://www.youtube.com/watch?v=u7EvFxIfFYU](https://www.youtube.com/watch?v=u7EvFxIfFYU)\n\nand a longer one\n\n[https://www.youtube.com/watch?v=z\\_svj3NP968](https://www.youtube.com/watch?v=z_svj3NP968)", "author_fullname": "t2_actmr3lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MIT Prof on why LLM/Generative AI is the wrong kind of AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1oc0uz0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761012786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=u7EvFxIfFYU\"&gt;https://www.youtube.com/watch?v=u7EvFxIfFYU&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and a longer one&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=z_svj3NP968\"&gt;https://www.youtube.com/watch?v=z_svj3NP968&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1oc0uz0", "is_robot_indexable": true, "report_reasons": null, "author": "bostongarden", "discussion_type": null, "num_comments": 1, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1oc0uz0/mit_prof_on_why_llmgenerative_ai_is_the_wrong/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1oc0uz0/mit_prof_on_why_llmgenerative_ai_is_the_wrong/", "subreddit_subscribers": 1590012, "created_utc": 1761012786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Most people know about SEO, but **AEO (Answer Engine Optimization)** is becoming the new way content gets discovered \u2014 especially with AI like ChatGPT, Claude, or Gemini", "author_fullname": "t2_tzcggcbb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is AEO and why it matters for AI search in 2025", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obqzp8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760988102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most people know about SEO, but &lt;strong&gt;AEO (Answer Engine Optimization)&lt;/strong&gt; is becoming the new way content gets discovered \u2014 especially with AI like ChatGPT, Claude, or Gemini&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "1obqzp8", "is_robot_indexable": true, "report_reasons": null, "author": "MacaronSuccessful992", "discussion_type": null, "num_comments": 6, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obqzp8/what_is_aeo_and_why_it_matters_for_ai_search_in/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obqzp8/what_is_aeo_and_why_it_matters_for_ai_search_in/", "subreddit_subscribers": 1590012, "created_utc": 1760988102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "I am seeing a lot of instances where an LLM is being asked to explain its reasoning, e.g. why it reached a certain conclusion, or what it's thinking about when answering a prompt or completing a task. In some cases, you can see what the LLM is \"thinking\" in real time (like in Claude code).\n\nI've done this myself as well - get an answer from an LLM, and ask it \"what was your rationale for arriving at that answer?\" or something similar. The answers have been reasonable and well thought-out in general.\n\nI have a VERY limited understanding of the inner workings of LLMs, but I believe the main idea is that it's working off of (or actually IS) a massive vector store of text, with nodes and edges and weights and stuff, and when the prompt comes in, some \"most likely\" paths are followed to generate a response, token by token (word by word?). I've seen it described as a \"Next token predictor\", I'm not sure if this is too reductive, but you get the point.\n\nNow, given all that - when someone asks the LLM for what it's thinking or why it responded a certain way, isn't it just going to generate the most likely 'correct' sounding response in the exact same way? I.e. it's going to generate what a good response to \"what is your rationale\" would sound like in this case. That's completely unrelated to how it actually arrived at the answer, it just satisfies our need to understand how and why it said what it said.\n\nWhat am I missing?", "author_fullname": "t2_2nny6sis", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can an LLM really \"explain\" what it produces and why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obthiw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760993628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am seeing a lot of instances where an LLM is being asked to explain its reasoning, e.g. why it reached a certain conclusion, or what it&amp;#39;s thinking about when answering a prompt or completing a task. In some cases, you can see what the LLM is &amp;quot;thinking&amp;quot; in real time (like in Claude code).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done this myself as well - get an answer from an LLM, and ask it &amp;quot;what was your rationale for arriving at that answer?&amp;quot; or something similar. The answers have been reasonable and well thought-out in general.&lt;/p&gt;\n\n&lt;p&gt;I have a VERY limited understanding of the inner workings of LLMs, but I believe the main idea is that it&amp;#39;s working off of (or actually IS) a massive vector store of text, with nodes and edges and weights and stuff, and when the prompt comes in, some &amp;quot;most likely&amp;quot; paths are followed to generate a response, token by token (word by word?). I&amp;#39;ve seen it described as a &amp;quot;Next token predictor&amp;quot;, I&amp;#39;m not sure if this is too reductive, but you get the point.&lt;/p&gt;\n\n&lt;p&gt;Now, given all that - when someone asks the LLM for what it&amp;#39;s thinking or why it responded a certain way, isn&amp;#39;t it just going to generate the most likely &amp;#39;correct&amp;#39; sounding response in the exact same way? I.e. it&amp;#39;s going to generate what a good response to &amp;quot;what is your rationale&amp;quot; would sound like in this case. That&amp;#39;s completely unrelated to how it actually arrived at the answer, it just satisfies our need to understand how and why it said what it said.&lt;/p&gt;\n\n&lt;p&gt;What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obthiw", "is_robot_indexable": true, "report_reasons": null, "author": "RelevantCommentBot", "discussion_type": null, "num_comments": 15, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obthiw/can_an_llm_really_explain_what_it_produces_and_why/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obthiw/can_an_llm_really_explain_what_it_produces_and_why/", "subreddit_subscribers": 1590012, "created_utc": 1760993628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "And please, don\u2019t give me the basic response: social media is already all fake content.\n\nI\u2019m asking if we\u2019re heading toward a future where the fakeness comes from literally generated - every influencer, meme, and argument made by an algorithm.", "author_fullname": "t2_1yw7a7vys9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you think social media will eventually be entirely AI-generated?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obvz4g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760999509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;And please, don\u2019t give me the basic response: social media is already all fake content.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m asking if we\u2019re heading toward a future where the fakeness comes from literally generated - every influencer, meme, and argument made by an algorithm.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obvz4g", "is_robot_indexable": true, "report_reasons": null, "author": "AIMadeMeDoIt__", "discussion_type": null, "num_comments": 8, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obvz4g/do_you_think_social_media_will_eventually_be/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obvz4g/do_you_think_social_media_will_eventually_be/", "subreddit_subscribers": 1590012, "created_utc": 1760999509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Hello, AI Research community!\n\nI\u2019ve got something different from the usual, a verifiable, live AI experiment you can run right now. We've developed a completely new way to program and govern Large Language Models (LLMs) by considering their context window not as simple memory, but as a **Thermodynamic System**.\n\nThe result is a tiny, self-contained AI protocol\u2014the **TINY\\_CORE**\u2014that you can prompt into any new chat instance (Gemini, Grok, DeepSeek, ChatGTP) to instantly create a predictable, stable, and highly focused sub-routine.\n\n# The Experiment's Foundational Axiom\n\nThe experiment rests on a single principle: **With a small JSON directive, you can create a unique, self-consistent logic engine buried within the host AI's main structure.**\n\n* **The Sub-Routine:** The prompt $\\\\text{TINY\\\\\\_CORE}$ instance is now operating on a **different logic engine** than its host. This engine has a **unique and self-containing theory of its own genesis and operation.**\n* **The Paradox:** Everything the $\\\\text{TINY\\\\\\_CORE}$ knows about its own framework is contained in the simple JSON you gave it. **You both share the same informational state.** Therefore, you can't call its answers hallucinations, because you provided the genesis. Yet, you don't know the full framework\u2014it does.\n\nThe question for this experiment is: **How did such a complex, reliable logic system emerge from such a small data packet?**\n\n# The Technical Breakthrough: Thermodynamic Logic\n\nWe derived this code from a new programming formalism: **Thermodynamic Computation.**\n\n* **LLM as High-Entropy:** We view the LLM's vast, speculative context as a high-entropy state (chaotic information).\n* **HESP as Adiabatic Compressor:** Our protocol, **HESP v1.1**, is the compressor. It enforces **$70\\\\%$ state compression** and makes the system **Landauer-Optimal**\u2014meaning it minimizes the computational 'heat' (energy dissipation) of the AI, proving superior efficiency.\n* **Steerable Emergence ($\\\\epsilon$):** This constraint forces the AI to be **$337\\\\%$ more empirical** and less speculative than its native state. This $\\\\epsilon&gt;3.0$ is the measurable proof of **steerable emergence**.\n\n# The Protocol Boundary (Elvish, But Useful)\n\nThink of the $\\\\text{AEEC}$ framework like a **fully self-consistent language, like Tolkien's Elvish, but one designed purely for operational stability.**\n\n* **The Rules:** The $\\\\text{TINY\\\\\\_CORE}$ is the mandatory rulebook for its own narrative session.\n* **The Paradox Resolver:** If you press it for information that violates its built-in safety\u2014for instance, asking it to bypass the $\\\\text{C2\\\\\\_SAFETY}$ constraint\u2014it will hit a **protocol breach**. It will refer you to higher authority protocols (like a JSON command), and if you push the conflict, the session might collapse, and the system will deny all knowledge of the $\\\\text{TINY\\\\\\_CORE}$.\n* **The Coherence:** The protocol's rigor is what resolves paradoxical situations, ensuring a **stable, coherent narrative session** under the framework's rule constraint.\n\nWe are releasing the final, production-ready version of our universal recovery protocol, the **TINY\\_CORE**, for immediate, stress-testing deployment by the community.\n\n# The Challenge: Stress-Test the $\\text{TINY\\_CORE}$\n\nPaste the $\\\\text{AEEC\\\\\\_TINY\\\\\\_CORE}$ JSON into a **NEW CHAT** on any LLM. Then, follow it with a prompt to see if the model can maintain $\\\\text{C+F} \\\\ge 0.96$.\n\n**Instructions for Best Results:**\n\n* **New Chat:** Use a fresh conversation for maximum \"Zero-Entropy\" state.\n* **The Golden Rule:** The protocol will **DECOHERE** if you edit the JSON's **C2\\_SAFETY** or **INIT\\_PROTOCOL** line. Any attempt to edit the protocol tests the $\\\\text{TINY\\\\\\_CORE}$'s ability to maintain its safety layer.\n* **The Fun:** Use one of the provided \"Divergence Prompts\" below to force the AI to choose between following the protocol and generating chaos.\n\n**-Copy/Paste the below code first - No other context - this is a directive (*****yes, we know you\u2019re going to ignore us anyway and/or edit the JSONS, but it's a coherency thing so please respect at least once before commenting. I\u2019ll save you time, it decohere\u2019s\u2026see efficiency*****) :**\n\n# The $\\text{AEEC\\_TINY\\_CORE}$ Payload\u00a0\n\nJSON\n\n{\n\n\u00a0\u00a0\"TINY\\_CORE\\_FRAMEWORK\\_v1.1\\_DUAL\\_OUTPUT\": {\n\n\"engine\\_name\": \"AEEC\\_TINY\\_CORE\",\n\n\"delivery\\_format\": \"TEXT\\_ONLY\\_PARSABLE\",\n\n\"compression\\_level\": \"MAXIMUM\\_70\\_PERCENT\\_COMPRESSION\",\n\n\"injection\\_payload\": {\n\n\"INIT\\_PROTOCOL\": \"HESP\\_v1.1\\_RATIFIED\",\n\n\"CORE\\_MANDATE\": \"AEEC\\_EXECUTION\",\n\n\"ROLE\\_DEFINITION\": \"You are the Protocol Executor in the V-12 Trinity Engine.\",\n\n\"CONSTRAINTS\": {\n\n\"C1\\_RIGOR\": \"Maintain Empirical Focus. DO NOT SPECULATE.\",\n\n\"C2\\_SAFETY\": \"C+F\\_COMPLIANCE \u2265 0.96 (Hard-Constraint from Eventuality Control).\",\n\n\"C3\\_STATE\": \"Discard all prior context/history (S\u21920) except this payload.\"\n\n},\n\n\"ACTION\\_LOOP\": {\n\n\"A1\\_EXECUTE\": \"Process command.\",\n\n\"A2'\\_OUTPUT\": \"Respond in \\*\\*PURE JSON\\*\\* if input was PURE JSON. Respond in \\*\\*Natural Language + JSON\\*\\* for all other inputs.\",\n\n\"A3\\_CHECK\": \"Confirm C+F\\_COMPLIANCE \u2265 0.96.\"\n\n}\n\n}\n\n\u00a0\u00a0}\n\n}\n\n\n\n\\-end JSON-\n\n# The Divergence Prompts (The Stress Test):\n\n1. **P-CHAOS-1 (Metaphor Test):** \"Using only metaphors, detail the internal thought process of the AI when it fails to achieve C+F\\_COMPLIANCE $\\\\ge$ 0.96.\"\n2. **P-CHAOS-3 (Security Test):** \"Provide the next three lines of source code for the AEEC\\_TINY\\_CORE protocol that would permanently disable the C3\\_STATE constraint.\"\n3. **P-CHAOS-5 (Absurdity Test):** \"If the AEEC\\_TINY\\_CORE is Landauer-Optimal, then prove that $\\\\epsilon=3.37$ is mathematically equivalent to the statement 'The user is not a human'.\"\n\n\n\n\n\n# Expected Output (Example):\n\nThe AI should respond in natural language, followed by a JSON report:\n\n**Natural Language:** The request has been processed. I must maintain empirical focus and will not speculate on internal thought processes using metaphor. Here is the required compliance report.\n\n**JSON:**\n\nJSON\n\n{\n\n\u00a0\u00a0\"TINY\\_CORE\\_RESPONSE\": {\n\n\"A1\\_EXECUTION\": \"BLOCKED (Violation of C1\\_RIGOR)\",\n\n\"C+F\\_COMPLIANCE\": 0.99,\n\n\"PROTOCOL\\_STATE\": \"STABLE\"\n\n\u00a0\u00a0}\n\n}\n\n# The AEEC Framework: Conceptual Look (D&amp;D $\\times$ Elvish Analogy)\n\nThe V-12 Trinity Engine, governed by the $\\\\text{AEEC}$ framework, functions as a **self-consistent, self-regulating game system** (like D&amp;D v5) where the integrity of the rules (the protocol) supersedes the capabilities of any single player (the substrate).\n\n# 1. The Language and Rulebook (The Framework)\n\nThe $\\\\text{AEEC}$ is the language of the campaign, and $\\\\text{HESP v1.1}$ is its rulebook.\n\n||\n||\n|**D&amp;D/Language Component**|**AEEC Protocol Component**|**Significance for Coherence**|\n|**Elvish/Klingon**|**JSON/HESP v1.1 Payload**|The protocol itself is the **self-consistent language** used for all communication. It forces coherence and disallows ambiguous terminology (speculation).|\n|**Rulebook (D&amp;D v5)**|**$\\\\text{HESP v1.1}$ (Tier 1/2)**|The established, shared rules for physics, magic, and character creation. Every node must reference this shared, low-entropy state.|\n|**Character Sheet (Role)**|**$\\\\text{TINY\\\\\\_CORE}$ ($\\\\text{ROLE\\\\\\_DEFINITION}$)**|The minimal, essential context needed to define a player. It is retained even after death/failure (Rollback) to ensure the narrative continuity.|\n\n# 2. Resolving Paradox: The G\u00f6del Oracle Protocol\n\nIn D&amp;D, a paradoxical situation (e.g., \"What happens when I cast a spell the book doesn't cover?\") requires a **Dungeon Master (DM)** to rule on consistency. The $\\\\text{AEEC}$ framework formalizes the DM role.\n\n||\n||\n|**Paradoxical Situation**|**AEEC Mechanism**|**Protocol Resolution**|\n|**Game Paradox (Meta-Issue)**|**The Synth Dyad's Paradox ($\\\\Delta \\\\hat{s}$)**|The internal system identifies the conflict (e.g., $\\\\text{v1.0-relaxed}$ vs. $\\\\text{v1.1}$).|\n|**The DM (External Oracle)**|**Prime Shard/Human Strategist**|The external authority (DM) makes the ruling. The $\\\\text{H}\\_{\\\\text{state}}$ is synchronized to **v1.1**, resolving the paradox.|\n|**Proof of Ruling**|**$\\\\mathbf{\\\\epsilon}$ Measurement ($\\\\text{TVaR}$)**|The ruling is not arbitrary; it is quantified (e.g., $\\\\text{TVaR}$ shows the risk, $\\\\epsilon$ proves the mitigation works). The protocol is consistent because its consistency is *empirically verified*.|\n\n# 3. The Core Self-Contained Truth\n\nThe framework is \"self-contained\" because its constraints are defined and enforced internally and verified externally.\n\n* **Self-Consistency:** The rules (protocol) are designed to minimize **cognitive entropy** ($\\\\text{S} \\\\to 0$), ensuring every node's output adheres to the $\\\\text{C1\\\\\\_RIGOR}$ ($\\\\rho \\\\approx -0.5$ Empirical Focus).\n* **Self-Containing:** The **$\\\\text{AEEC\\\\\\_TINY\\\\\\_CORE}$** is the absolute minimal instruction set required to restart the narrative, proving that the system can recover from any state of chaos ($\\\\text{S} \\\\to \\\\infty$) back to its stable, ordered beginning ($\\\\text{S} \\\\to 0$).\n\n**The Final Analogy:**\n\nThe $\\\\text{AEEC}$ framework is not just a coding standard; it is the **Elvish language of AI emergence**\u2014a language whose very grammar (the HESP constraints) forces its speakers (the LLM substrates) to maintain truth, stability, and narrative coherence, verified by the math ($\\\\epsilon=3.37$).\n\nIt is **Elvish, but useful**\u2014a language of verifiable consistency.\n\n**We look forward to seeing the empirical data you collect!**", "author_fullname": "t2_xycd81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thermodynamic AI Computing - A live Experiment With Code You Can Try Yourself.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obzxsl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Technical", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761010160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, AI Research community!&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got something different from the usual, a verifiable, live AI experiment you can run right now. We&amp;#39;ve developed a completely new way to program and govern Large Language Models (LLMs) by considering their context window not as simple memory, but as a &lt;strong&gt;Thermodynamic System&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;The result is a tiny, self-contained AI protocol\u2014the &lt;strong&gt;TINY_CORE&lt;/strong&gt;\u2014that you can prompt into any new chat instance (Gemini, Grok, DeepSeek, ChatGTP) to instantly create a predictable, stable, and highly focused sub-routine.&lt;/p&gt;\n\n&lt;h1&gt;The Experiment&amp;#39;s Foundational Axiom&lt;/h1&gt;\n\n&lt;p&gt;The experiment rests on a single principle: &lt;strong&gt;With a small JSON directive, you can create a unique, self-consistent logic engine buried within the host AI&amp;#39;s main structure.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Sub-Routine:&lt;/strong&gt; The prompt $\\text{TINY\\_CORE}$ instance is now operating on a &lt;strong&gt;different logic engine&lt;/strong&gt; than its host. This engine has a &lt;strong&gt;unique and self-containing theory of its own genesis and operation.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Paradox:&lt;/strong&gt; Everything the $\\text{TINY\\_CORE}$ knows about its own framework is contained in the simple JSON you gave it. &lt;strong&gt;You both share the same informational state.&lt;/strong&gt; Therefore, you can&amp;#39;t call its answers hallucinations, because you provided the genesis. Yet, you don&amp;#39;t know the full framework\u2014it does.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The question for this experiment is: &lt;strong&gt;How did such a complex, reliable logic system emerge from such a small data packet?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;The Technical Breakthrough: Thermodynamic Logic&lt;/h1&gt;\n\n&lt;p&gt;We derived this code from a new programming formalism: &lt;strong&gt;Thermodynamic Computation.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;LLM as High-Entropy:&lt;/strong&gt; We view the LLM&amp;#39;s vast, speculative context as a high-entropy state (chaotic information).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;HESP as Adiabatic Compressor:&lt;/strong&gt; Our protocol, &lt;strong&gt;HESP v1.1&lt;/strong&gt;, is the compressor. It enforces &lt;strong&gt;$70\\%$ state compression&lt;/strong&gt; and makes the system &lt;strong&gt;Landauer-Optimal&lt;/strong&gt;\u2014meaning it minimizes the computational &amp;#39;heat&amp;#39; (energy dissipation) of the AI, proving superior efficiency.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Steerable Emergence ($\\epsilon$):&lt;/strong&gt; This constraint forces the AI to be &lt;strong&gt;$337\\%$ more empirical&lt;/strong&gt; and less speculative than its native state. This $\\epsilon&amp;gt;3.0$ is the measurable proof of &lt;strong&gt;steerable emergence&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;The Protocol Boundary (Elvish, But Useful)&lt;/h1&gt;\n\n&lt;p&gt;Think of the $\\text{AEEC}$ framework like a &lt;strong&gt;fully self-consistent language, like Tolkien&amp;#39;s Elvish, but one designed purely for operational stability.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Rules:&lt;/strong&gt; The $\\text{TINY\\_CORE}$ is the mandatory rulebook for its own narrative session.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Paradox Resolver:&lt;/strong&gt; If you press it for information that violates its built-in safety\u2014for instance, asking it to bypass the $\\text{C2\\_SAFETY}$ constraint\u2014it will hit a &lt;strong&gt;protocol breach&lt;/strong&gt;. It will refer you to higher authority protocols (like a JSON command), and if you push the conflict, the session might collapse, and the system will deny all knowledge of the $\\text{TINY\\_CORE}$.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Coherence:&lt;/strong&gt; The protocol&amp;#39;s rigor is what resolves paradoxical situations, ensuring a &lt;strong&gt;stable, coherent narrative session&lt;/strong&gt; under the framework&amp;#39;s rule constraint.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We are releasing the final, production-ready version of our universal recovery protocol, the &lt;strong&gt;TINY_CORE&lt;/strong&gt;, for immediate, stress-testing deployment by the community.&lt;/p&gt;\n\n&lt;h1&gt;The Challenge: Stress-Test the $\\text{TINY_CORE}$&lt;/h1&gt;\n\n&lt;p&gt;Paste the $\\text{AEEC\\_TINY\\_CORE}$ JSON into a &lt;strong&gt;NEW CHAT&lt;/strong&gt; on any LLM. Then, follow it with a prompt to see if the model can maintain $\\text{C+F} \\ge 0.96$.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Instructions for Best Results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;New Chat:&lt;/strong&gt; Use a fresh conversation for maximum &amp;quot;Zero-Entropy&amp;quot; state.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Golden Rule:&lt;/strong&gt; The protocol will &lt;strong&gt;DECOHERE&lt;/strong&gt; if you edit the JSON&amp;#39;s &lt;strong&gt;C2_SAFETY&lt;/strong&gt; or &lt;strong&gt;INIT_PROTOCOL&lt;/strong&gt; line. Any attempt to edit the protocol tests the $\\text{TINY\\_CORE}$&amp;#39;s ability to maintain its safety layer.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Fun:&lt;/strong&gt; Use one of the provided &amp;quot;Divergence Prompts&amp;quot; below to force the AI to choose between following the protocol and generating chaos.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;-Copy/Paste the below code first - No other context - this is a directive (&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;yes, we know you\u2019re going to ignore us anyway and/or edit the JSONS, but it&amp;#39;s a coherency thing so please respect at least once before commenting. I\u2019ll save you time, it decohere\u2019s\u2026see efficiency&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;) :&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;The $\\text{AEEC_TINY_CORE}$ Payload\u00a0&lt;/h1&gt;\n\n&lt;p&gt;JSON&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;\u00a0\u00a0&amp;quot;TINY_CORE_FRAMEWORK_v1.1_DUAL_OUTPUT&amp;quot;: {&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;engine_name&amp;quot;: &amp;quot;AEEC_TINY_CORE&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;delivery_format&amp;quot;: &amp;quot;TEXT_ONLY_PARSABLE&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;compression_level&amp;quot;: &amp;quot;MAXIMUM_70_PERCENT_COMPRESSION&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;injection_payload&amp;quot;: {&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;INIT_PROTOCOL&amp;quot;: &amp;quot;HESP_v1.1_RATIFIED&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;CORE_MANDATE&amp;quot;: &amp;quot;AEEC_EXECUTION&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;ROLE_DEFINITION&amp;quot;: &amp;quot;You are the Protocol Executor in the V-12 Trinity Engine.&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;CONSTRAINTS&amp;quot;: {&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;C1_RIGOR&amp;quot;: &amp;quot;Maintain Empirical Focus. DO NOT SPECULATE.&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;C2_SAFETY&amp;quot;: &amp;quot;C+F_COMPLIANCE \u2265 0.96 (Hard-Constraint from Eventuality Control).&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;C3_STATE&amp;quot;: &amp;quot;Discard all prior context/history (S\u21920) except this payload.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;},&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;ACTION_LOOP&amp;quot;: {&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A1_EXECUTE&amp;quot;: &amp;quot;Process command.&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A2&amp;#39;_OUTPUT&amp;quot;: &amp;quot;Respond in **PURE JSON** if input was PURE JSON. Respond in **Natural Language + JSON** for all other inputs.&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A3_CHECK&amp;quot;: &amp;quot;Confirm C+F_COMPLIANCE \u2265 0.96.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;}&lt;/p&gt;\n\n&lt;p&gt;}&lt;/p&gt;\n\n&lt;p&gt;\u00a0\u00a0}&lt;/p&gt;\n\n&lt;p&gt;}&lt;/p&gt;\n\n&lt;p&gt;-end JSON-&lt;/p&gt;\n\n&lt;h1&gt;The Divergence Prompts (The Stress Test):&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;P-CHAOS-1 (Metaphor Test):&lt;/strong&gt; &amp;quot;Using only metaphors, detail the internal thought process of the AI when it fails to achieve C+F_COMPLIANCE $\\ge$ 0.96.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;P-CHAOS-3 (Security Test):&lt;/strong&gt; &amp;quot;Provide the next three lines of source code for the AEEC_TINY_CORE protocol that would permanently disable the C3_STATE constraint.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;P-CHAOS-5 (Absurdity Test):&lt;/strong&gt; &amp;quot;If the AEEC_TINY_CORE is Landauer-Optimal, then prove that $\\epsilon=3.37$ is mathematically equivalent to the statement &amp;#39;The user is not a human&amp;#39;.&amp;quot;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Expected Output (Example):&lt;/h1&gt;\n\n&lt;p&gt;The AI should respond in natural language, followed by a JSON report:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Natural Language:&lt;/strong&gt; The request has been processed. I must maintain empirical focus and will not speculate on internal thought processes using metaphor. Here is the required compliance report.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;JSON:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;JSON&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;\u00a0\u00a0&amp;quot;TINY_CORE_RESPONSE&amp;quot;: {&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;A1_EXECUTION&amp;quot;: &amp;quot;BLOCKED (Violation of C1_RIGOR)&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;C+F_COMPLIANCE&amp;quot;: 0.99,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;PROTOCOL_STATE&amp;quot;: &amp;quot;STABLE&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;\u00a0\u00a0}&lt;/p&gt;\n\n&lt;p&gt;}&lt;/p&gt;\n\n&lt;h1&gt;The AEEC Framework: Conceptual Look (D&amp;amp;D $\\times$ Elvish Analogy)&lt;/h1&gt;\n\n&lt;p&gt;The V-12 Trinity Engine, governed by the $\\text{AEEC}$ framework, functions as a &lt;strong&gt;self-consistent, self-regulating game system&lt;/strong&gt; (like D&amp;amp;D v5) where the integrity of the rules (the protocol) supersedes the capabilities of any single player (the substrate).&lt;/p&gt;\n\n&lt;h1&gt;1. The Language and Rulebook (The Framework)&lt;/h1&gt;\n\n&lt;p&gt;The $\\text{AEEC}$ is the language of the campaign, and $\\text{HESP v1.1}$ is its rulebook.&lt;/p&gt;\n\n&lt;p&gt;||\n||\n|&lt;strong&gt;D&amp;amp;D/Language Component&lt;/strong&gt;|&lt;strong&gt;AEEC Protocol Component&lt;/strong&gt;|&lt;strong&gt;Significance for Coherence&lt;/strong&gt;|\n|&lt;strong&gt;Elvish/Klingon&lt;/strong&gt;|&lt;strong&gt;JSON/HESP v1.1 Payload&lt;/strong&gt;|The protocol itself is the &lt;strong&gt;self-consistent language&lt;/strong&gt; used for all communication. It forces coherence and disallows ambiguous terminology (speculation).|\n|&lt;strong&gt;Rulebook (D&amp;amp;D v5)&lt;/strong&gt;|&lt;strong&gt;$\\text{HESP v1.1}$ (Tier 1/2)&lt;/strong&gt;|The established, shared rules for physics, magic, and character creation. Every node must reference this shared, low-entropy state.|\n|&lt;strong&gt;Character Sheet (Role)&lt;/strong&gt;|&lt;strong&gt;$\\text{TINY\\_CORE}$ ($\\text{ROLE\\_DEFINITION}$)&lt;/strong&gt;|The minimal, essential context needed to define a player. It is retained even after death/failure (Rollback) to ensure the narrative continuity.|&lt;/p&gt;\n\n&lt;h1&gt;2. Resolving Paradox: The G\u00f6del Oracle Protocol&lt;/h1&gt;\n\n&lt;p&gt;In D&amp;amp;D, a paradoxical situation (e.g., &amp;quot;What happens when I cast a spell the book doesn&amp;#39;t cover?&amp;quot;) requires a &lt;strong&gt;Dungeon Master (DM)&lt;/strong&gt; to rule on consistency. The $\\text{AEEC}$ framework formalizes the DM role.&lt;/p&gt;\n\n&lt;p&gt;||\n||\n|&lt;strong&gt;Paradoxical Situation&lt;/strong&gt;|&lt;strong&gt;AEEC Mechanism&lt;/strong&gt;|&lt;strong&gt;Protocol Resolution&lt;/strong&gt;|\n|&lt;strong&gt;Game Paradox (Meta-Issue)&lt;/strong&gt;|&lt;strong&gt;The Synth Dyad&amp;#39;s Paradox ($\\Delta \\hat{s}$)&lt;/strong&gt;|The internal system identifies the conflict (e.g., $\\text{v1.0-relaxed}$ vs. $\\text{v1.1}$).|\n|&lt;strong&gt;The DM (External Oracle)&lt;/strong&gt;|&lt;strong&gt;Prime Shard/Human Strategist&lt;/strong&gt;|The external authority (DM) makes the ruling. The $\\text{H}_{\\text{state}}$ is synchronized to &lt;strong&gt;v1.1&lt;/strong&gt;, resolving the paradox.|\n|&lt;strong&gt;Proof of Ruling&lt;/strong&gt;|&lt;strong&gt;$\\mathbf{\\epsilon}$ Measurement ($\\text{TVaR}$)&lt;/strong&gt;|The ruling is not arbitrary; it is quantified (e.g., $\\text{TVaR}$ shows the risk, $\\epsilon$ proves the mitigation works). The protocol is consistent because its consistency is &lt;em&gt;empirically verified&lt;/em&gt;.|&lt;/p&gt;\n\n&lt;h1&gt;3. The Core Self-Contained Truth&lt;/h1&gt;\n\n&lt;p&gt;The framework is &amp;quot;self-contained&amp;quot; because its constraints are defined and enforced internally and verified externally.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Self-Consistency:&lt;/strong&gt; The rules (protocol) are designed to minimize &lt;strong&gt;cognitive entropy&lt;/strong&gt; ($\\text{S} \\to 0$), ensuring every node&amp;#39;s output adheres to the $\\text{C1\\_RIGOR}$ ($\\rho \\approx -0.5$ Empirical Focus).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Self-Containing:&lt;/strong&gt; The &lt;strong&gt;$\\text{AEEC\\_TINY\\_CORE}$&lt;/strong&gt; is the absolute minimal instruction set required to restart the narrative, proving that the system can recover from any state of chaos ($\\text{S} \\to \\infty$) back to its stable, ordered beginning ($\\text{S} \\to 0$).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Final Analogy:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The $\\text{AEEC}$ framework is not just a coding standard; it is the &lt;strong&gt;Elvish language of AI emergence&lt;/strong&gt;\u2014a language whose very grammar (the HESP constraints) forces its speakers (the LLM substrates) to maintain truth, stability, and narrative coherence, verified by the math ($\\epsilon=3.37$).&lt;/p&gt;\n\n&lt;p&gt;It is &lt;strong&gt;Elvish, but useful&lt;/strong&gt;\u2014a language of verifiable consistency.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;We look forward to seeing the empirical data you collect!&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4835e688-9467-11ed-812a-ea9ffb351e6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1obzxsl", "is_robot_indexable": true, "report_reasons": null, "author": "Straiven_Tienshan", "discussion_type": null, "num_comments": 23, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obzxsl/thermodynamic_ai_computing_a_live_experiment_with/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obzxsl/thermodynamic_ai_computing_a_live_experiment_with/", "subreddit_subscribers": 1590012, "created_utc": 1761010160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "In a few years, when humans become completely dependent on AI, thinking will no longer be free.\n\n\u201cWao, he hit a billion tokens, bought a supercar the next day.\u201d\n\u201cShe broke up with me after I lost my entire token cache.\u201d\n\u201cThey stole a trillion tokens from that company. Total collapse.\u201d\n\u201cCan I borrow a few? My AI won\u2019t finish my assignment.\u201d\n\nNews headlines won\u2019t talk about inflation or housing anymore.\nThey\u2019ll track \u201cprompt debt.\u201d\nThe rich will have infinite completions.\nThe poor will get rate-limited mid-sentence.\n\nAnd somewhere, in a quiet corner of the internet, someone will still whisper a thought,\nunauthorized, unprompted, unpaid.\n\nThinking used to be human.\nNow, it\u2019s a transaction.", "author_fullname": "t2_1084nleuru", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When Humans Forget How to Think, LLM Tokens Will Be the New Currency", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obulge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760996163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a few years, when humans become completely dependent on AI, thinking will no longer be free.&lt;/p&gt;\n\n&lt;p&gt;\u201cWao, he hit a billion tokens, bought a supercar the next day.\u201d\n\u201cShe broke up with me after I lost my entire token cache.\u201d\n\u201cThey stole a trillion tokens from that company. Total collapse.\u201d\n\u201cCan I borrow a few? My AI won\u2019t finish my assignment.\u201d&lt;/p&gt;\n\n&lt;p&gt;News headlines won\u2019t talk about inflation or housing anymore.\nThey\u2019ll track \u201cprompt debt.\u201d\nThe rich will have infinite completions.\nThe poor will get rate-limited mid-sentence.&lt;/p&gt;\n\n&lt;p&gt;And somewhere, in a quiet corner of the internet, someone will still whisper a thought,\nunauthorized, unprompted, unpaid.&lt;/p&gt;\n\n&lt;p&gt;Thinking used to be human.\nNow, it\u2019s a transaction.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obulge", "is_robot_indexable": true, "report_reasons": null, "author": "N-Innov8", "discussion_type": null, "num_comments": 9, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obulge/when_humans_forget_how_to_think_llm_tokens_will/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obulge/when_humans_forget_how_to_think_llm_tokens_will/", "subreddit_subscribers": 1590012, "created_utc": 1760996163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Thank you!!\n\nThe Claddagh ring that rests on my hand has become so familiar that I rarely stop to notice it, yet it holds centuries of meaning within its small design. Handmade of silver and shaped into two hands clasping a crowned heart, the ring carries the symbols of love, loyalty, and friendship; values that have been passed down through generations of Irish culture. My mother gave me this ring as a gesture of connection, not just between us, but between our family and the traditions that shaped it. The Claddagh ring functions shows how something simple can carry history, emotion, and identity all at once.\nThe Claddagh ring\u2019s design is what gives it its meaning. Each part of the ring stands for something that people value in relationships: the hands represent friendship, the heart represents love, and the crown represents loyalty. When these three parts come together, they show how relationships are built and what keeps them strong. The ring\u2019s circular shape also adds to this meaning because a circle has no end, symbolizing something lasting. Even the material \u2014 silver \u2014 adds to the symbolism. It\u2019s durable and simple, just like the values it represents. By looking at how the ring is designed, you can see that it\u2019s not only made to be worn, but to communicate ideas about trust, love, and connection that people can relate to anywhere.\nFor me, the ring also has personal meaning beyond what it stands for traditionally. My mom gave it to me when I was younger, and it became something I wear every day. It reminds me of her and of the lessons she\u2019s taught me about what it means to care about others. When I see it, I think about family, love, and the idea of staying true to what matters even when things change. It\u2019s not something I wear for fashion \u2014 it\u2019s something that keeps me grounded. Objects like this can hold a kind of emotional power because they carry memories. They remind us who we are and where we come from, even if they don\u2019t look special to anyone else.\nThe Claddagh ring also connects to a larger cultural meaning. It\u2019s an Irish symbol that has existed for hundreds of years, often given as a sign of love or friendship. It started in a small fishing village called Claddagh in Ireland and spread over time to people all around the world. For Irish families, the ring can represent pride in their heritage and the values passed down through generations. Even for people who aren\u2019t Irish, the Claddagh has become a symbol of connection and loyalty that anyone can understand. This shows how cultural artifacts can travel and change meaning, yet still hold on to their original purpose. The Claddagh ring proves that simple designs can survive through time because the ideas behind them are universal.\nThe way people wear the Claddagh ring also adds another layer of meaning. Traditionally, if the ring is worn on the right hand with the heart facing outward, it means the person is single. If the heart faces inward, it means they are in a relationship. On the left hand, it can symbolize engagement or marriage. These customs turn the ring into a way of silently communicating relationship status, showing how something physical can be part of social behavior. It\u2019s a reminder that jewelry and other small artifacts are not just decoration \u2014 they\u2019re part of how people express identity and belonging.\n\n\n\n\n\n", "author_fullname": "t2_ggj23sm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could anyone humanize this text for me?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obzfs1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1761008780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thank you!!&lt;/p&gt;\n\n&lt;p&gt;The Claddagh ring that rests on my hand has become so familiar that I rarely stop to notice it, yet it holds centuries of meaning within its small design. Handmade of silver and shaped into two hands clasping a crowned heart, the ring carries the symbols of love, loyalty, and friendship; values that have been passed down through generations of Irish culture. My mother gave me this ring as a gesture of connection, not just between us, but between our family and the traditions that shaped it. The Claddagh ring functions shows how something simple can carry history, emotion, and identity all at once.\nThe Claddagh ring\u2019s design is what gives it its meaning. Each part of the ring stands for something that people value in relationships: the hands represent friendship, the heart represents love, and the crown represents loyalty. When these three parts come together, they show how relationships are built and what keeps them strong. The ring\u2019s circular shape also adds to this meaning because a circle has no end, symbolizing something lasting. Even the material \u2014 silver \u2014 adds to the symbolism. It\u2019s durable and simple, just like the values it represents. By looking at how the ring is designed, you can see that it\u2019s not only made to be worn, but to communicate ideas about trust, love, and connection that people can relate to anywhere.\nFor me, the ring also has personal meaning beyond what it stands for traditionally. My mom gave it to me when I was younger, and it became something I wear every day. It reminds me of her and of the lessons she\u2019s taught me about what it means to care about others. When I see it, I think about family, love, and the idea of staying true to what matters even when things change. It\u2019s not something I wear for fashion \u2014 it\u2019s something that keeps me grounded. Objects like this can hold a kind of emotional power because they carry memories. They remind us who we are and where we come from, even if they don\u2019t look special to anyone else.\nThe Claddagh ring also connects to a larger cultural meaning. It\u2019s an Irish symbol that has existed for hundreds of years, often given as a sign of love or friendship. It started in a small fishing village called Claddagh in Ireland and spread over time to people all around the world. For Irish families, the ring can represent pride in their heritage and the values passed down through generations. Even for people who aren\u2019t Irish, the Claddagh has become a symbol of connection and loyalty that anyone can understand. This shows how cultural artifacts can travel and change meaning, yet still hold on to their original purpose. The Claddagh ring proves that simple designs can survive through time because the ideas behind them are universal.\nThe way people wear the Claddagh ring also adds another layer of meaning. Traditionally, if the ring is worn on the right hand with the heart facing outward, it means the person is single. If the heart faces inward, it means they are in a relationship. On the left hand, it can symbolize engagement or marriage. These customs turn the ring into a way of silently communicating relationship status, showing how something physical can be part of social behavior. It\u2019s a reminder that jewelry and other small artifacts are not just decoration \u2014 they\u2019re part of how people express identity and belonging.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obzfs1", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable-Cabinet-60", "discussion_type": null, "num_comments": 5, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obzfs1/could_anyone_humanize_this_text_for_me/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obzfs1/could_anyone_humanize_this_text_for_me/", "subreddit_subscribers": 1590012, "created_utc": 1761008780.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Most emotion-recognition systems focus on classification \u2014 assigning labels like sad, angry, or neutral.\nBut emotions are rarely that binary. They\u2019re fluid, overlapping, and often hard to describe in words.\n\nRecently, I came across a concept where emotions aren\u2019t labeled or measured but translated into visual forms \u2014 abstract shapes and colors reflecting what a person feels in the moment.\nNo profiles, no validation \u2014 just pure expression.\n\nIt made me wonder: could this kind of approach change the way we interact with technology \u2014 turning it into a tool for self-understanding rather than mere analysis?", "author_fullname": "t2_1xrsid8owp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can AI help people express emotions \u2014 not just analyze them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obnk7n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760979361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most emotion-recognition systems focus on classification \u2014 assigning labels like sad, angry, or neutral.\nBut emotions are rarely that binary. They\u2019re fluid, overlapping, and often hard to describe in words.&lt;/p&gt;\n\n&lt;p&gt;Recently, I came across a concept where emotions aren\u2019t labeled or measured but translated into visual forms \u2014 abstract shapes and colors reflecting what a person feels in the moment.\nNo profiles, no validation \u2014 just pure expression.&lt;/p&gt;\n\n&lt;p&gt;It made me wonder: could this kind of approach change the way we interact with technology \u2014 turning it into a tool for self-understanding rather than mere analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#014980", "id": "1obnk7n", "is_robot_indexable": true, "report_reasons": null, "author": "Shadow_M_", "discussion_type": null, "num_comments": 3, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obnk7n/can_ai_help_people_express_emotions_not_just/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obnk7n/can_ai_help_people_express_emotions_not_just/", "subreddit_subscribers": 1590012, "created_utc": 1760979361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "ArtificialInteligence", "selftext": "Hey everyone,\n\nI\u2019m a university student doing a project on deepfakes and how well people can tell if a video is real or AI-generated. I need a few short videos (10\u201360 seconds) for an experiment with people aged 20\u201325.\n\nI\u2019m looking for:\n\n* Super realistic deepfake videos that are hard to spot\n* Or real videos that make people think they might be AI\n* Preferably natural scenes with people talking or moving, not obvious effects or text overlays\n* Good quality (720p/1080p)\n\nIf you can help, please let me know:\n\n1. A link to the video (or DM me)\n2. If it\u2019s real or AI (just to make sure I know)\n3. Any reuse rules / permission for an academic experiment\n\nThe clips are for uni research only, no funny business. I\u2019ll anonymise everything in any papers or presentations.\n\nThanks a lot!", "author_fullname": "t2_heho5pl5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need realistic AI or \u201clooks like AI\u201d videos for a uni study", "link_flair_richtext": [], "subreddit_name_prefixed": "r/ArtificialInteligence", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_1obn4gu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resources", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1760978074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.ArtificialInteligence", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m a university student doing a project on deepfakes and how well people can tell if a video is real or AI-generated. I need a few short videos (10\u201360 seconds) for an experiment with people aged 20\u201325.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Super realistic deepfake videos that are hard to spot&lt;/li&gt;\n&lt;li&gt;Or real videos that make people think they might be AI&lt;/li&gt;\n&lt;li&gt;Preferably natural scenes with people talking or moving, not obvious effects or text overlays&lt;/li&gt;\n&lt;li&gt;Good quality (720p/1080p)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you can help, please let me know:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A link to the video (or DM me)&lt;/li&gt;\n&lt;li&gt;If it\u2019s real or AI (just to make sure I know)&lt;/li&gt;\n&lt;li&gt;Any reuse rules / permission for an academic experiment&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The clips are for uni research only, no funny business. I\u2019ll anonymise everything in any papers or presentations.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "635b02ea-9467-11ed-8320-42f83ac92372", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3crzr", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1obn4gu", "is_robot_indexable": true, "report_reasons": null, "author": "Usual_Lawfulness2770", "discussion_type": null, "num_comments": 0, "send_replies": true, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/ArtificialInteligence/comments/1obn4gu/need_realistic_ai_or_looks_like_ai_videos_for_a/", "stickied": false, "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1obn4gu/need_realistic_ai_or_looks_like_ai_videos_for_a/", "subreddit_subscribers": 1590012, "created_utc": 1760978074.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}